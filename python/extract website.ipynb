{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93717fc",
   "metadata": {},
   "source": [
    "# Website Content Extraction\n",
    "\n",
    "This notebook extracts all content from the old website HTML and organizes it into:\n",
    "- Individual markdown files per section\n",
    "- Downloaded images organized by section folders\n",
    "\n",
    "## Sections to Extract:\n",
    "1. Hero section\n",
    "2. Before/After with handles\n",
    "3. Before/After stats only\n",
    "4. Client testimonials/results\n",
    "5. How it works\n",
    "6. FAQ\n",
    "7. Pricing/Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbdd6c",
   "metadata": {},
   "source": [
    "## Setup & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8963ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Directories created\n",
      "  - Base: /Users/marclamy/Documents - Local/Code/external/pblaunch/python/extracted_content\n",
      "  - Images: /Users/marclamy/Documents - Local/Code/external/pblaunch/python/extracted_content/images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Setup directories\n",
    "base_dir = Path('extracted_content')\n",
    "images_dir = base_dir / 'images'\n",
    "\n",
    "# Create directory structure\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "images_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Image tracking\n",
    "image_manifest = defaultdict(list)\n",
    "\n",
    "print(\"‚úì Directories created\")\n",
    "print(f\"  - Base: {base_dir.absolute()}\")\n",
    "print(f\"  - Images: {images_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7bc72e",
   "metadata": {},
   "source": [
    "## Load & Parse HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "642c73fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HTML loaded and parsed\n",
      "  - File size: 640,176 characters\n",
      "  - Total elements: 1770\n"
     ]
    }
   ],
   "source": [
    "# Load the HTML file\n",
    "html_file = Path('old-website.html')\n",
    "\n",
    "with open(html_file, 'r', encoding='utf-8') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "print(f\"‚úì HTML loaded and parsed\")\n",
    "print(f\"  - File size: {len(html_content):,} characters\")\n",
    "print(f\"  - Total elements: {len(soup.find_all())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfcb3b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4f4b4",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5971133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def download_image(url, section_name, img_index):\n",
    "    \"\"\"Download an image from URL and save it to the appropriate section folder\"\"\"\n",
    "    if not url or url.startswith('data:'):\n",
    "        # Skip data URLs (base64 encoded images)\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create section folder\n",
    "        section_folder = images_dir / section_name\n",
    "        section_folder.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Get file extension from URL or default to jpg\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        ext = Path(parsed.path).suffix or '.jpg'\n",
    "        if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']:\n",
    "            ext = '.jpg'\n",
    "        \n",
    "        # Create filename\n",
    "        filename = f\"{section_name}-{img_index}{ext}\"\n",
    "        filepath = section_folder / filename\n",
    "        \n",
    "        # Download if URL is complete\n",
    "        if url.startswith('http'):\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            rel_path = f\"images/{section_name}/{filename}\"\n",
    "            image_manifest[section_name].append(rel_path)\n",
    "            return rel_path\n",
    "        else:\n",
    "            print(f\"  ‚ö† Skipping relative URL: {url[:100]}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error downloading {url[:100]}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(element):\n",
    "    \"\"\"Extract and clean text from an element\"\"\"\n",
    "    if not element:\n",
    "        return \"\"\n",
    "    return element.get_text(strip=True, separator=' ')\n",
    "\n",
    "def find_section_by_text(soup, text_snippet):\n",
    "    \"\"\"Find a section containing specific text\"\"\"\n",
    "    elements = soup.find_all(string=re.compile(text_snippet, re.IGNORECASE))\n",
    "    if elements:\n",
    "        # Return the parent section\n",
    "        for elem in elements:\n",
    "            parent = elem.find_parent(['div', 'section'])\n",
    "            if parent:\n",
    "                return parent\n",
    "    return None\n",
    "\n",
    "print(\"‚úì Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44093f11",
   "metadata": {},
   "source": [
    "## Extract Hero Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe7cfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Hero Section...\n",
      "  Found 0 images\n",
      "‚úì Hero section extracted\n",
      "  - Heading: GROW & SELL ON SOCIAL MEDIA...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting Hero Section...\")\n",
    "\n",
    "# Find hero section by looking for main heading\n",
    "hero_section = find_section_by_text(soup, \"GROW & SELL ON SOCIAL MEDIA\")\n",
    "\n",
    "hero_content = {\n",
    "    'heading': '',\n",
    "    'subheading': '',\n",
    "    'description': '',\n",
    "    'cta': '',\n",
    "    'images': []\n",
    "}\n",
    "\n",
    "if hero_section:\n",
    "    # Extract headings\n",
    "    headings = hero_section.find_all(['h1', 'h2', 'h3'])\n",
    "    if headings:\n",
    "        hero_content['heading'] = extract_text(headings[0])\n",
    "        if len(headings) > 1:\n",
    "            hero_content['subheading'] = extract_text(headings[1])\n",
    "    \n",
    "    # Extract description/paragraph text\n",
    "    paragraphs = hero_section.find_all('p')\n",
    "    if paragraphs:\n",
    "        hero_content['description'] = '\\n\\n'.join([extract_text(p) for p in paragraphs if extract_text(p)])\n",
    "    \n",
    "    # Extract CTA buttons/links\n",
    "    buttons = hero_section.find_all(['a', 'button'])\n",
    "    cta_texts = [extract_text(btn) for btn in buttons if extract_text(btn)]\n",
    "    if cta_texts:\n",
    "        hero_content['cta'] = cta_texts[0]\n",
    "    \n",
    "    # Download images\n",
    "    images = hero_section.find_all('img')\n",
    "    print(f\"  Found {len(images)} images\")\n",
    "    for idx, img in enumerate(images, 1):\n",
    "        img_url = img.get('src') or img.get('data-src')\n",
    "        if img_url:\n",
    "            downloaded = download_image(img_url, 'hero', idx)\n",
    "            if downloaded:\n",
    "                hero_content['images'].append(downloaded)\n",
    "                print(f\"  ‚úì Downloaded: {downloaded}\")\n",
    "\n",
    "print(f\"‚úì Hero section extracted\")\n",
    "print(f\"  - Heading: {hero_content['heading'][:50]}...\" if hero_content['heading'] else \"  - No heading found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d1e08",
   "metadata": {},
   "source": [
    "## Extract Before/After Sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf85a4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Before/After Sections...\n",
      "  Scanning 84 total images for Instagram screenshots...\n",
      "‚úì Before/After sections extracted\n",
      "  - With handles: 0 images\n",
      "  - Stats only: 0 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting Before/After Sections...\")\n",
    "\n",
    "# Look for sections with \"CLIENT BEFORE & AFTER\" or similar text\n",
    "before_after_section = find_section_by_text(soup, \"CLIENT BEFORE.*AFTER\")\n",
    "\n",
    "before_after_handles = {\n",
    "    'title': '',\n",
    "    'items': [],\n",
    "    'images': []\n",
    "}\n",
    "\n",
    "before_after_stats = {\n",
    "    'title': '',\n",
    "    'items': [],\n",
    "    'images': []\n",
    "}\n",
    "\n",
    "# Find all images that look like Instagram screenshots\n",
    "all_images = soup.find_all('img')\n",
    "instagram_images = []\n",
    "\n",
    "print(f\"  Scanning {len(all_images)} total images for Instagram screenshots...\")\n",
    "\n",
    "# Download all images from potential before/after sections\n",
    "img_counter_handles = 1\n",
    "img_counter_stats = 1\n",
    "\n",
    "for img in all_images:\n",
    "    img_url = img.get('src') or img.get('data-src')\n",
    "    if not img_url:\n",
    "        continue\n",
    "    \n",
    "    # Check if this looks like an Instagram screenshot or profile image\n",
    "    # Look at surrounding text for context\n",
    "    parent_text = extract_text(img.find_parent())\n",
    "    \n",
    "    # Heuristic: if near text with \"followers\", \"posts\", \"following\", it's likely before/after\n",
    "    if any(keyword in parent_text.lower() for keyword in ['follower', 'posts', 'following', '@']):\n",
    "        # If it has @ symbol or handle visible, it's the \"with handles\" type\n",
    "        if '@' in parent_text or any(word.startswith('@') for word in parent_text.split()):\n",
    "            downloaded = download_image(img_url, 'before-after-handles', img_counter_handles)\n",
    "            if downloaded:\n",
    "                before_after_handles['images'].append(downloaded)\n",
    "                print(f\"  ‚úì Before/After (with handles): {downloaded}\")\n",
    "                img_counter_handles += 1\n",
    "        else:\n",
    "            # Just stats, no handle\n",
    "            downloaded = download_image(img_url, 'before-after-stats', img_counter_stats)\n",
    "            if downloaded:\n",
    "                before_after_stats['images'].append(downloaded)\n",
    "                print(f\"  ‚úì Before/After (stats only): {downloaded}\")\n",
    "                img_counter_stats += 1\n",
    "\n",
    "# If we found a specific section, extract its title\n",
    "if before_after_section:\n",
    "    section_title = before_after_section.find(['h1', 'h2', 'h3'])\n",
    "    if section_title:\n",
    "        before_after_handles['title'] = extract_text(section_title)\n",
    "        before_after_stats['title'] = extract_text(section_title)\n",
    "\n",
    "print(f\"‚úì Before/After sections extracted\")\n",
    "print(f\"  - With handles: {len(before_after_handles['images'])} images\")\n",
    "print(f\"  - Stats only: {len(before_after_stats['images'])} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2debff0",
   "metadata": {},
   "source": [
    "## Extract Client Results & Testimonials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba8a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Client Results & Testimonials...\n",
      "  Found 0 images in testimonials section\n",
      "  Scanning for additional testimonial screenshots...\n",
      "‚úì Client results extracted\n",
      "  - Text testimonials: 0\n",
      "  - Image testimonials: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting Client Results & Testimonials...\")\n",
    "\n",
    "# Look for section with \"LEAD RESULTS\" or testimonial indicators\n",
    "client_results_section = find_section_by_text(soup, \"CLIENT.*LEAD.*RESULTS\")\n",
    "if not client_results_section:\n",
    "    client_results_section = find_section_by_text(soup, \"TESTIMONIAL\")\n",
    "\n",
    "client_results = {\n",
    "    'title': '',\n",
    "    'testimonials': [],\n",
    "    'images': []\n",
    "}\n",
    "\n",
    "# Extract title\n",
    "if client_results_section:\n",
    "    title_elem = client_results_section.find(['h1', 'h2', 'h3'])\n",
    "    if title_elem:\n",
    "        client_results['title'] = extract_text(title_elem)\n",
    "    \n",
    "    # Find testimonial text blocks\n",
    "    # Look for text blocks that look like testimonials (longer paragraphs, quotes, etc)\n",
    "    text_blocks = client_results_section.find_all(['p', 'blockquote', 'div'])\n",
    "    for block in text_blocks:\n",
    "        text = extract_text(block)\n",
    "        # If it's a substantial text block (testimonials tend to be longer)\n",
    "        if len(text) > 50 and any(keyword in text.lower() for keyword in ['thank', 'appreciate', 'great', 'helped', 'working', 'business']):\n",
    "            client_results['testimonials'].append(text)\n",
    "    \n",
    "    # Download testimonial screenshot images\n",
    "    images = client_results_section.find_all('img')\n",
    "    print(f\"  Found {len(images)} images in testimonials section\")\n",
    "    for idx, img in enumerate(images, 1):\n",
    "        img_url = img.get('src') or img.get('data-src')\n",
    "        if img_url:\n",
    "            downloaded = download_image(img_url, 'client-results', idx)\n",
    "            if downloaded:\n",
    "                client_results['images'].append(downloaded)\n",
    "                print(f\"  ‚úì Downloaded testimonial: {downloaded}\")\n",
    "\n",
    "# Also scan for testimonial-looking images throughout the page\n",
    "# (screenshots of messages, comments, etc.)\n",
    "print(\"  Scanning for additional testimonial screenshots...\")\n",
    "for img in all_images:\n",
    "    img_url = img.get('src') or img.get('data-src')\n",
    "    if not img_url:\n",
    "        continue\n",
    "    \n",
    "    # Check surrounding context for testimonial keywords\n",
    "    parent = img.find_parent()\n",
    "    if parent:\n",
    "        parent_text = extract_text(parent).lower()\n",
    "        # Look for message/testimonial indicators\n",
    "        if any(keyword in parent_text for keyword in ['sms', 'message', 'dm', 'thank you', 'appreciation']):\n",
    "            # Avoid duplicates\n",
    "            section_name = 'client-results'\n",
    "            if img_url not in [img.get('src') or img.get('data-src') for img in (client_results_section.find_all('img') if client_results_section else [])]:\n",
    "                idx = len(client_results['images']) + 1\n",
    "                downloaded = download_image(img_url, section_name, idx)\n",
    "                if downloaded and downloaded not in client_results['images']:\n",
    "                    client_results['images'].append(downloaded)\n",
    "                    print(f\"  ‚úì Additional testimonial: {downloaded}\")\n",
    "\n",
    "print(f\"‚úì Client results extracted\")\n",
    "print(f\"  - Text testimonials: {len(client_results['testimonials'])}\")\n",
    "print(f\"  - Image testimonials: {len(client_results['images'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18adeb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f318ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting How It Works Section...\n",
      "  Found 0 images\n",
      "‚úì How It Works extracted\n",
      "  - Steps: 0\n",
      "  - Images: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting How It Works Section...\")\n",
    "\n",
    "# Look for \"how it works\" or process section\n",
    "how_it_works_section = find_section_by_text(soup, \"HOW.*IT.*WORKS\")\n",
    "if not how_it_works_section:\n",
    "    how_it_works_section = find_section_by_text(soup, \"OUR.*PROCESS\")\n",
    "if not how_it_works_section:\n",
    "    how_it_works_section = find_section_by_text(soup, \"WHAT.*WE.*DO\")\n",
    "\n",
    "how_it_works = {\n",
    "    'title': '',\n",
    "    'steps': [],\n",
    "    'description': '',\n",
    "    'images': []\n",
    "}\n",
    "\n",
    "if how_it_works_section:\n",
    "    # Extract title\n",
    "    title_elem = how_it_works_section.find(['h1', 'h2', 'h3'])\n",
    "    if title_elem:\n",
    "        how_it_works['title'] = extract_text(title_elem)\n",
    "    \n",
    "    # Extract step-by-step content\n",
    "    # Look for numbered items, list items, or sections with step indicators\n",
    "    lists = how_it_works_section.find_all(['ol', 'ul'])\n",
    "    for lst in lists:\n",
    "        items = lst.find_all('li')\n",
    "        for item in items:\n",
    "            step_text = extract_text(item)\n",
    "            if step_text:\n",
    "                how_it_works['steps'].append(step_text)\n",
    "    \n",
    "    # If no list, look for headings + paragraphs pattern\n",
    "    if not how_it_works['steps']:\n",
    "        all_headings = how_it_works_section.find_all(['h3', 'h4', 'h5'])\n",
    "        for heading in all_headings:\n",
    "            heading_text = extract_text(heading)\n",
    "            # Find following paragraph\n",
    "            next_sibling = heading.find_next_sibling(['p', 'div'])\n",
    "            if next_sibling:\n",
    "                desc = extract_text(next_sibling)\n",
    "                if desc:\n",
    "                    how_it_works['steps'].append(f\"{heading_text}: {desc}\")\n",
    "                else:\n",
    "                    how_it_works['steps'].append(heading_text)\n",
    "            else:\n",
    "                how_it_works['steps'].append(heading_text)\n",
    "    \n",
    "    # Extract general description\n",
    "    paragraphs = how_it_works_section.find_all('p')\n",
    "    desc_parts = [extract_text(p) for p in paragraphs if extract_text(p) and len(extract_text(p)) > 30]\n",
    "    how_it_works['description'] = '\\n\\n'.join(desc_parts)\n",
    "    \n",
    "    # Download images\n",
    "    images = how_it_works_section.find_all('img')\n",
    "    print(f\"  Found {len(images)} images\")\n",
    "    for idx, img in enumerate(images, 1):\n",
    "        img_url = img.get('src') or img.get('data-src')\n",
    "        if img_url:\n",
    "            downloaded = download_image(img_url, 'how-it-works', idx)\n",
    "            if downloaded:\n",
    "                how_it_works['images'].append(downloaded)\n",
    "                print(f\"  ‚úì Downloaded: {downloaded}\")\n",
    "\n",
    "print(f\"‚úì How It Works extracted\")\n",
    "print(f\"  - Steps: {len(how_it_works['steps'])}\")\n",
    "print(f\"  - Images: {len(how_it_works['images'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9e12f",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b17003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FAQ Section...\n",
      "  Found 0 images\n",
      "‚úì FAQ extracted\n",
      "  - Q&A pairs: 0\n",
      "  - Images: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting FAQ Section...\")\n",
    "\n",
    "# Look for FAQ section\n",
    "faq_section = find_section_by_text(soup, \"FAQ\")\n",
    "if not faq_section:\n",
    "    faq_section = find_section_by_text(soup, \"FREQUENTLY.*ASKED\")\n",
    "if not faq_section:\n",
    "    faq_section = find_section_by_text(soup, \"QUESTIONS\")\n",
    "\n",
    "faq = {\n",
    "    'title': '',\n",
    "    'items': [],\n",
    "    'images': []\n",
    "}\n",
    "\n",
    "if faq_section:\n",
    "    # Extract title\n",
    "    title_elem = faq_section.find(['h1', 'h2', 'h3'])\n",
    "    if title_elem:\n",
    "        faq['title'] = extract_text(title_elem)\n",
    "    \n",
    "    # Extract Q&A pairs\n",
    "    # Look for accordion/collapsible patterns, dt/dd pairs, or heading+paragraph\n",
    "    \n",
    "    # Method 1: dt/dd (definition list)\n",
    "    dts = faq_section.find_all('dt')\n",
    "    dds = faq_section.find_all('dd')\n",
    "    if dts and dds:\n",
    "        for dt, dd in zip(dts, dds):\n",
    "            question = extract_text(dt)\n",
    "            answer = extract_text(dd)\n",
    "            if question and answer:\n",
    "                faq['items'].append({'question': question, 'answer': answer})\n",
    "    \n",
    "    # Method 2: Headings followed by paragraphs\n",
    "    if not faq['items']:\n",
    "        headings = faq_section.find_all(['h3', 'h4', 'h5', 'h6'])\n",
    "        for heading in headings:\n",
    "            question = extract_text(heading)\n",
    "            # Find the next paragraph or div\n",
    "            answer_elem = heading.find_next_sibling(['p', 'div'])\n",
    "            if answer_elem:\n",
    "                answer = extract_text(answer_elem)\n",
    "                if question and answer and len(answer) > 10:\n",
    "                    faq['items'].append({'question': question, 'answer': answer})\n",
    "    \n",
    "    # Method 3: Divs with question/answer classes\n",
    "    if not faq['items']:\n",
    "        faq_items = faq_section.find_all(['div', 'article'], class_=re.compile('faq|question|accordion', re.I))\n",
    "        for item in faq_items:\n",
    "            # Try to find question and answer within\n",
    "            q_elem = item.find(class_=re.compile('question|q|title', re.I))\n",
    "            a_elem = item.find(class_=re.compile('answer|a|content', re.I))\n",
    "            if q_elem and a_elem:\n",
    "                question = extract_text(q_elem)\n",
    "                answer = extract_text(a_elem)\n",
    "                if question and answer:\n",
    "                    faq['items'].append({'question': question, 'answer': answer})\n",
    "    \n",
    "    # Download images\n",
    "    images = faq_section.find_all('img')\n",
    "    print(f\"  Found {len(images)} images\")\n",
    "    for idx, img in enumerate(images, 1):\n",
    "        img_url = img.get('src') or img.get('data-src')\n",
    "        if img_url:\n",
    "            downloaded = download_image(img_url, 'faq', idx)\n",
    "            if downloaded:\n",
    "                faq['images'].append(downloaded)\n",
    "                print(f\"  ‚úì Downloaded: {downloaded}\")\n",
    "\n",
    "print(f\"‚úì FAQ extracted\")\n",
    "print(f\"  - Q&A pairs: {len(faq['items'])}\")\n",
    "print(f\"  - Images: {len(faq['images'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7008075",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f369128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Pricing Section...\n",
      "  Found 0 images\n",
      "‚úì Pricing extracted\n",
      "  - Packages: 0\n",
      "  - Images: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting Pricing Section...\")\n",
    "\n",
    "# Look for pricing/packages section\n",
    "pricing_section = find_section_by_text(soup, \"PRICING\")\n",
    "if not pricing_section:\n",
    "    pricing_section = find_section_by_text(soup, \"PACKAGES\")\n",
    "if not pricing_section:\n",
    "    pricing_section = find_section_by_text(soup, \"SOCIAL MEDIA MANAGEMENT\")\n",
    "\n",
    "pricing = {\n",
    "    'title': '',\n",
    "    'packages': [],\n",
    "    'description': '',\n",
    "    'images': []\n",
    "}\n",
    "\n",
    "if pricing_section:\n",
    "    # Extract title\n",
    "    title_elem = pricing_section.find(['h1', 'h2', 'h3'])\n",
    "    if title_elem:\n",
    "        pricing['title'] = extract_text(title_elem)\n",
    "    \n",
    "    # Look for pricing cards or package structures\n",
    "    # Common patterns: divs with price, features list, package names\n",
    "    \n",
    "    # Look for elements containing price indicators ($, /month, etc)\n",
    "    price_elements = pricing_section.find_all(string=re.compile(r'\\$|/month|per month|price', re.I))\n",
    "    \n",
    "    package_containers = []\n",
    "    for elem in price_elements:\n",
    "        container = elem.find_parent(['div', 'article', 'section'])\n",
    "        if container and container not in package_containers:\n",
    "            package_containers.append(container)\n",
    "    \n",
    "    # Extract package details\n",
    "    for container in package_containers:\n",
    "        package = {\n",
    "            'name': '',\n",
    "            'price': '',\n",
    "            'features': [],\n",
    "            'description': ''\n",
    "        }\n",
    "        \n",
    "        # Get package name (usually a heading)\n",
    "        name_elem = container.find(['h3', 'h4', 'h5'])\n",
    "        if name_elem:\n",
    "            package['name'] = extract_text(name_elem)\n",
    "        \n",
    "        # Get price\n",
    "        price_text = container.find(string=re.compile(r'\\$\\d+', re.I))\n",
    "        if price_text:\n",
    "            package['price'] = price_text.strip()\n",
    "        \n",
    "        # Get features (usually in a list)\n",
    "        features_list = container.find(['ul', 'ol'])\n",
    "        if features_list:\n",
    "            items = features_list.find_all('li')\n",
    "            package['features'] = [extract_text(item) for item in items if extract_text(item)]\n",
    "        \n",
    "        # Get description\n",
    "        paragraphs = container.find_all('p')\n",
    "        if paragraphs:\n",
    "            package['description'] = ' '.join([extract_text(p) for p in paragraphs if extract_text(p)])\n",
    "        \n",
    "        if package['name'] or package['price']:\n",
    "            pricing['packages'].append(package)\n",
    "    \n",
    "    # Extract general description\n",
    "    paragraphs = pricing_section.find_all('p')\n",
    "    desc_parts = [extract_text(p) for p in paragraphs if extract_text(p) and len(extract_text(p)) > 30]\n",
    "    if desc_parts:\n",
    "        pricing['description'] = '\\n\\n'.join(desc_parts[:2])  # First 2 paragraphs\n",
    "    \n",
    "    # Download images\n",
    "    images = pricing_section.find_all('img')\n",
    "    print(f\"  Found {len(images)} images\")\n",
    "    for idx, img in enumerate(images, 1):\n",
    "        img_url = img.get('src') or img.get('data-src')\n",
    "        if img_url:\n",
    "            downloaded = download_image(img_url, 'pricing', idx)\n",
    "            if downloaded:\n",
    "                pricing['images'].append(downloaded)\n",
    "                print(f\"  ‚úì Downloaded: {downloaded}\")\n",
    "\n",
    "print(f\"‚úì Pricing extracted\")\n",
    "print(f\"  - Packages: {len(pricing['packages'])}\")\n",
    "print(f\"  - Images: {len(pricing['images'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460b686",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf7bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Markdown Files...\n",
      "  ‚úì Created: hero.md\n",
      "  ‚úì Created: before-after-handles.md\n",
      "  ‚úì Created: before-after-stats.md\n",
      "‚úì Before/After sections written\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Markdown Files...\")\n",
    "\n",
    "def write_markdown(filename, content):\n",
    "    \"\"\"Write content to a markdown file\"\"\"\n",
    "    filepath = base_dir / filename\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"  ‚úì Created: {filename}\")\n",
    "    return filepath\n",
    "\n",
    "# 1. Hero Section\n",
    "hero_md = f\"\"\"# {hero_content.get('heading', 'Hero Section')}\n",
    "\n",
    "{hero_content.get('subheading', '')}\n",
    "\n",
    "## Description\n",
    "\n",
    "{hero_content.get('description', '')}\n",
    "\n",
    "## Call to Action\n",
    "\n",
    "{hero_content.get('cta', '')}\n",
    "\n",
    "## Images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for img_path in hero_content.get('images', []):\n",
    "    hero_md += f\"![Hero Image]({img_path})\\n\\n\"\n",
    "\n",
    "write_markdown('hero.md', hero_md)\n",
    "\n",
    "# 2. Before/After with Handles\n",
    "ba_handles_md = f\"\"\"# {before_after_handles.get('title', 'Client Before & After Results')}\n",
    "\n",
    "## Instagram Profile Growth (With Handles)\n",
    "\n",
    "This section shows before and after Instagram profiles with visible handles and metrics.\n",
    "\n",
    "## Images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for img_path in before_after_handles.get('images', []):\n",
    "    ba_handles_md += f\"![Before/After Profile]({img_path})\\n\\n\"\n",
    "\n",
    "if not before_after_handles.get('images'):\n",
    "    ba_handles_md += \"*No images found for this section. Check the website manually.*\\n\\n\"\n",
    "\n",
    "write_markdown('before-after-handles.md', ba_handles_md)\n",
    "\n",
    "# 3. Before/After Stats Only\n",
    "ba_stats_md = f\"\"\"# {before_after_stats.get('title', 'Client Growth Statistics')}\n",
    "\n",
    "## Instagram Profile Growth (Stats Only)\n",
    "\n",
    "This section shows before and after follower counts without visible handles.\n",
    "\n",
    "## Images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for img_path in before_after_stats.get('images', []):\n",
    "    ba_stats_md += f\"![Before/After Stats]({img_path})\\n\\n\"\n",
    "\n",
    "if not before_after_stats.get('images'):\n",
    "    ba_stats_md += \"*No images found for this section. Check the website manually.*\\n\\n\"\n",
    "\n",
    "write_markdown('before-after-stats.md', ba_stats_md)\n",
    "\n",
    "print(\"‚úì Before/After sections written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48af98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Created: client-results.md\n",
      "  ‚úì Created: how-it-works.md\n",
      "‚úì Client results and How It Works written\n"
     ]
    }
   ],
   "source": [
    "# 4. Client Results & Testimonials\n",
    "testimonials_md = f\"\"\"# {client_results.get('title', 'Client Results & Testimonials')}\n",
    "\n",
    "## Testimonial Texts\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if client_results.get('testimonials'):\n",
    "    for idx, testimonial in enumerate(client_results['testimonials'], 1):\n",
    "        testimonials_md += f\"### Testimonial {idx}\\n\\n\"\n",
    "        testimonials_md += f\"> {testimonial}\\n\\n\"\n",
    "else:\n",
    "    testimonials_md += \"*Note: Testimonial text will be extracted from screenshots below.*\\n\\n\"\n",
    "\n",
    "testimonials_md += \"## Testimonial Screenshots\\n\\n\"\n",
    "testimonials_md += \"*These images contain client feedback, messages, and results. Transcribe text from these images later.*\\n\\n\"\n",
    "\n",
    "for img_path in client_results.get('images', []):\n",
    "    testimonials_md += f\"![Client Testimonial]({img_path})\\n\\n\"\n",
    "\n",
    "if not client_results.get('images'):\n",
    "    testimonials_md += \"*No testimonial images found.*\\n\\n\"\n",
    "\n",
    "write_markdown('client-results.md', testimonials_md)\n",
    "\n",
    "# 5. How It Works\n",
    "how_it_works_md = f\"\"\"# {how_it_works.get('title', 'How It Works')}\n",
    "\n",
    "## Description\n",
    "\n",
    "{how_it_works.get('description', '')}\n",
    "\n",
    "## Process Steps\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if how_it_works.get('steps'):\n",
    "    for idx, step in enumerate(how_it_works['steps'], 1):\n",
    "        how_it_works_md += f\"{idx}. {step}\\n\\n\"\n",
    "else:\n",
    "    how_it_works_md += \"*No process steps found. Content may need to be extracted manually.*\\n\\n\"\n",
    "\n",
    "how_it_works_md += \"## Images\\n\\n\"\n",
    "\n",
    "for img_path in how_it_works.get('images', []):\n",
    "    how_it_works_md += f\"![Process Image]({img_path})\\n\\n\"\n",
    "\n",
    "write_markdown('how-it-works.md', how_it_works_md)\n",
    "\n",
    "print(\"‚úì Client results and How It Works written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85297168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Created: faq.md\n",
      "  ‚úì Created: pricing.md\n",
      "‚úì FAQ and Pricing written\n"
     ]
    }
   ],
   "source": [
    "# 6. FAQ\n",
    "faq_md = f\"\"\"# {faq.get('title', 'Frequently Asked Questions')}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if faq.get('items'):\n",
    "    for idx, item in enumerate(faq['items'], 1):\n",
    "        faq_md += f\"## {item.get('question', f'Question {idx}')}\\n\\n\"\n",
    "        faq_md += f\"{item.get('answer', '')}\\n\\n\"\n",
    "else:\n",
    "    faq_md += \"*No FAQ items found. Content may need to be extracted manually from the HTML.*\\n\\n\"\n",
    "\n",
    "if faq.get('images'):\n",
    "    faq_md += \"## Images\\n\\n\"\n",
    "    for img_path in faq['images']:\n",
    "        faq_md += f\"![FAQ Image]({img_path})\\n\\n\"\n",
    "\n",
    "write_markdown('faq.md', faq_md)\n",
    "\n",
    "# 7. Pricing\n",
    "pricing_md = f\"\"\"# {pricing.get('title', 'Pricing & Packages')}\n",
    "\n",
    "## Overview\n",
    "\n",
    "{pricing.get('description', '')}\n",
    "\n",
    "## Packages\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if pricing.get('packages'):\n",
    "    for pkg in pricing['packages']:\n",
    "        pricing_md += f\"### {pkg.get('name', 'Package')}\\n\\n\"\n",
    "        \n",
    "        if pkg.get('price'):\n",
    "            pricing_md += f\"**Price:** {pkg['price']}\\n\\n\"\n",
    "        \n",
    "        if pkg.get('description'):\n",
    "            pricing_md += f\"{pkg['description']}\\n\\n\"\n",
    "        \n",
    "        if pkg.get('features'):\n",
    "            pricing_md += \"**Features:**\\n\\n\"\n",
    "            for feature in pkg['features']:\n",
    "                pricing_md += f\"- {feature}\\n\"\n",
    "            pricing_md += \"\\n\"\n",
    "        \n",
    "        pricing_md += \"---\\n\\n\"\n",
    "else:\n",
    "    pricing_md += \"*No pricing packages found. Content may need to be extracted manually.*\\n\\n\"\n",
    "\n",
    "if pricing.get('images'):\n",
    "    pricing_md += \"## Images\\n\\n\"\n",
    "    for img_path in pricing['images']:\n",
    "        pricing_md += f\"![Pricing Image]({img_path})\\n\\n\"\n",
    "\n",
    "write_markdown('pricing.md', pricing_md)\n",
    "\n",
    "print(\"‚úì FAQ and Pricing written\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a809b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0171832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE!\n",
      "============================================================\n",
      "\n",
      "‚úì Image manifest saved: extracted_content/image_manifest.json\n",
      "\n",
      "üìÅ Created Files:\n",
      "------------------------------------------------------------\n",
      "  ‚úì 1 - hero.md (125 bytes)\n",
      "  ‚úì 2 - client before and after result.md (0 bytes)\n",
      "  ‚úì before-after-handles.md (214 bytes)\n",
      "  ‚úì before-after-stats.md (200 bytes)\n",
      "  ‚úì client-results.md (278 bytes)\n",
      "  ‚úì faq.md (87 bytes)\n",
      "  ‚úì hero.md (108 bytes)\n",
      "  ‚úì how-it-works.md (133 bytes)\n",
      "  ‚úì pricing.md (137 bytes)\n",
      "\n",
      "üì∑ Downloaded Images by Section:\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä Statistics:\n",
      "------------------------------------------------------------\n",
      "  Total markdown files: 9\n",
      "  Total images downloaded: 0\n",
      "  Total sections: 0\n",
      "\n",
      "üìÇ Output Location:\n",
      "------------------------------------------------------------\n",
      "  /Users/marclamy/Documents - Local/Code/external/pblaunch/python/extracted_content\n",
      "\n",
      "============================================================\n",
      "Next Steps:\n",
      "============================================================\n",
      "1. Review the markdown files for completeness\n",
      "2. Check downloaded images\n",
      "3. Manually transcribe text from screenshot images\n",
      "4. Use this content to create the new website design\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION COMPLETE!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Save image manifest\n",
    "manifest_path = base_dir / 'image_manifest.json'\n",
    "with open(manifest_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dict(image_manifest), f, indent=2)\n",
    "print(f\"‚úì Image manifest saved: {manifest_path}\\n\")\n",
    "\n",
    "# List all created files\n",
    "print(\"üìÅ Created Files:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "markdown_files = list(base_dir.glob('*.md'))\n",
    "for md_file in sorted(markdown_files):\n",
    "    size = md_file.stat().st_size\n",
    "    print(f\"  ‚úì {md_file.name} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nüì∑ Downloaded Images by Section:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for section, images in sorted(image_manifest.items()):\n",
    "    print(f\"  {section}: {len(images)} images\")\n",
    "    for img in images:\n",
    "        print(f\"    - {img}\")\n",
    "\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total markdown files: {len(markdown_files)}\")\n",
    "print(f\"  Total images downloaded: {sum(len(imgs) for imgs in image_manifest.values())}\")\n",
    "print(f\"  Total sections: {len(image_manifest)}\")\n",
    "\n",
    "print(f\"\\nüìÇ Output Location:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  {base_dir.absolute()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Review the markdown files for completeness\")\n",
    "print(\"2. Check downloaded images\")\n",
    "print(\"3. Manually transcribe text from screenshot images\")\n",
    "print(\"4. Use this content to create the new website design\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25cf68c",
   "metadata": {},
   "source": [
    "## Download ALL Images (Fallback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133540d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ALL images as fallback...\n",
      "This ensures we capture all images even if section detection missed them.\n",
      "\n",
      "Found 83 unique image URLs\n",
      "  [1/83] Downloading image-001.png... ‚úì (2,362 bytes)\n",
      "  [2/83] Downloading image-002.png... ‚úì (2,342 bytes)\n",
      "  [3/83] Downloading image-003.png... ‚úì (3,772 bytes)\n",
      "  [4/83] Downloading image-004.png... ‚úì (5,416 bytes)\n",
      "  [5/83] Downloading image-005.png... ‚úì (12,536 bytes)\n",
      "  [6/83] Downloading image-006.jpeg... ‚úì (13,358 bytes)\n",
      "  [7/83] Downloading image-007.jpeg... ‚úì (10,978 bytes)\n",
      "  [8/83] Downloading image-008.jpeg... ‚úì (12,106 bytes)\n",
      "  [9/83] Downloading image-009.jpeg... ‚úì (88,632 bytes)\n",
      "  [10/83] Downloading image-010.jpeg... ‚úì (38,720 bytes)\n",
      "  [11/83] Downloading image-011.png... ‚úì (27,204 bytes)\n",
      "  [12/83] Downloading image-012.jpeg... ‚úì (25,842 bytes)\n",
      "  [13/83] Downloading image-013.jpeg... ‚úì (48,906 bytes)\n",
      "  [14/83] Downloading image-014.jpeg... ‚úì (12,706 bytes)\n",
      "  [15/83] Downloading image-015.jpeg... ‚úì (25,558 bytes)\n",
      "  [16/83] Downloading image-016.jpeg... ‚úì (22,778 bytes)\n",
      "  [17/83] Downloading image-017.png... ‚úì (7,078 bytes)\n",
      "  [18/83] Downloading image-018.jpeg... ‚úì (15,182 bytes)\n",
      "  [19/83] Downloading image-019.png... ‚úì (7,932 bytes)\n",
      "  [20/83] Downloading image-020.png... ‚úì (9,128 bytes)\n",
      "  [21/83] Downloading image-021.png... ‚úì (7,678 bytes)\n",
      "  [22/83] Downloading image-022.png... ‚úì (9,638 bytes)\n",
      "  [23/83] Downloading image-023.png... ‚úì (8,604 bytes)\n",
      "  [24/83] Downloading image-024.png... ‚úì (10,564 bytes)\n",
      "  [25/83] Downloading image-025.png... ‚úì (12,518 bytes)\n",
      "  [26/83] Downloading image-026.png... ‚úì (9,872 bytes)\n",
      "  [27/83] Downloading image-027.png... ‚úì (14,130 bytes)\n",
      "  [28/83] Downloading image-028.png... ‚úì (11,494 bytes)\n",
      "  [29/83] Downloading image-029.png... ‚úì (11,594 bytes)\n",
      "  [30/83] Downloading image-030.png... ‚úì (11,844 bytes)\n",
      "  [31/83] Downloading image-031.png... ‚úì (9,858 bytes)\n",
      "  [32/83] Downloading image-032.png... ‚úì (13,324 bytes)\n",
      "  [33/83] Downloading image-033.png... ‚úì (11,930 bytes)\n",
      "  [34/83] Downloading image-034.png... ‚úì (10,444 bytes)\n",
      "  [35/83] Downloading image-035.png... ‚úì (11,434 bytes)\n",
      "  [36/83] Downloading image-036.png... ‚úì (10,078 bytes)\n",
      "  [37/83] Downloading image-037.png... ‚úì (9,712 bytes)\n",
      "  [38/83] Downloading image-038.png... ‚úì (12,744 bytes)\n",
      "  [39/83] Downloading image-039.png... ‚úì (10,820 bytes)\n",
      "  [40/83] Downloading image-040.png... ‚úì (8,400 bytes)\n",
      "  [41/83] Downloading image-041.png... ‚úì (7,822 bytes)\n",
      "  [42/83] Downloading image-042.png... ‚úì (12,032 bytes)\n",
      "  [43/83] Downloading image-043.png... ‚úì (12,658 bytes)\n",
      "  [44/83] Downloading image-044.png... ‚úì (10,216 bytes)\n",
      "  [45/83] Downloading image-045.png... ‚úì (10,694 bytes)\n",
      "  [46/83] Downloading image-046.png... ‚úì (11,224 bytes)\n",
      "  [47/83] Downloading image-047.png... ‚úì (14,632 bytes)\n",
      "  [48/83] Downloading image-048.png... ‚úì (12,320 bytes)\n",
      "  [49/83] Downloading image-049.png... ‚úì (13,116 bytes)\n",
      "  [50/83] Downloading image-050.png... ‚úì (7,938 bytes)\n",
      "  [51/83] Downloading image-051.png... ‚úì (9,756 bytes)\n",
      "  [52/83] Downloading image-052.png... ‚úì (10,162 bytes)\n",
      "  [53/83] Downloading image-053.png... ‚úì (12,248 bytes)\n",
      "  [54/83] Downloading image-054.png... ‚úì (14,934 bytes)\n",
      "  [55/83] Downloading image-055.png... ‚úì (15,262 bytes)\n",
      "  [56/83] Downloading image-056.png... ‚úì (10,876 bytes)\n",
      "  [57/83] Downloading image-057.png... ‚úì (12,930 bytes)\n",
      "  [58/83] Downloading image-058.png... ‚úì (10,752 bytes)\n",
      "  [59/83] Downloading image-059.png... ‚úì (13,744 bytes)\n",
      "  [60/83] Downloading image-060.png... ‚úì (12,122 bytes)\n",
      "  [61/83] Downloading image-061.png... ‚úì (14,150 bytes)\n",
      "  [62/83] Downloading image-062.png... ‚úì (13,248 bytes)\n",
      "  [63/83] Downloading image-063.jpeg... ‚úì (73,468 bytes)\n",
      "  [64/83] Downloading image-064.jpeg... ‚úì (19,472 bytes)\n",
      "  [65/83] Downloading image-065.png... ‚úì (7,812 bytes)\n",
      "  [66/83] Downloading image-066.png... ‚úì (8,984 bytes)\n",
      "  [67/83] Downloading image-067.png... ‚úì (8,890 bytes)\n",
      "  [68/83] Downloading image-068.png... ‚úì (10,728 bytes)\n",
      "  [69/83] Downloading image-069.jpeg... ‚úì (61,988 bytes)\n",
      "  [70/83] Downloading image-070.png... ‚úì (16,594 bytes)\n",
      "  [71/83] Downloading image-071.jpeg... ‚úì (19,984 bytes)\n",
      "  [72/83] Downloading image-072.jpeg... ‚úì (38,720 bytes)\n",
      "  [73/83] Downloading image-073.png... ‚úì (34,590 bytes)\n",
      "  [74/83] Downloading image-074.png... ‚úì (37,050 bytes)\n",
      "  [75/83] Downloading image-075.png... ‚úì (11,280 bytes)\n",
      "  [76/83] Downloading image-076.png... ‚úì (19,780 bytes)\n",
      "  [77/83] Downloading image-077.png... ‚úì (12,326 bytes)\n",
      "  [78/83] Downloading image-078.png... ‚úì (12,842 bytes)\n",
      "  [79/83] Downloading image-079.jpeg... ‚úì (44,376 bytes)\n",
      "  [80/83] Downloading image-080.png... ‚úì (15,710 bytes)\n",
      "  [81/83] Downloading image-081.jpeg... ‚úì (40,220 bytes)\n",
      "  [82/83] Downloading image-082.jpeg... ‚úì (34,960 bytes)\n",
      "  [83/83] Downloading image-083.jpeg... ‚úì (22,246 bytes)\n",
      "\n",
      "‚úì Image download complete!\n",
      "  Successfully downloaded: 83\n",
      "  Failed: 0\n",
      "  Location: /Users/marclamy/Documents - Local/Code/external/pblaunch/python/extracted_content/images/all-images\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading ALL images as fallback...\")\n",
    "print(\"This ensures we capture all images even if section detection missed them.\\n\")\n",
    "\n",
    "# Get all unique image URLs\n",
    "all_img_tags = soup.find_all('img')\n",
    "all_image_urls = set()\n",
    "\n",
    "for img in all_img_tags:\n",
    "    img_url = img.get('src') or img.get('data-src')\n",
    "    if img_url and img_url.startswith('http'):\n",
    "        all_image_urls.add(img_url)\n",
    "\n",
    "print(f\"Found {len(all_image_urls)} unique image URLs\")\n",
    "\n",
    "# Download all images to an 'all-images' folder\n",
    "all_images_section = 'all-images'\n",
    "all_images_folder = images_dir / all_images_section\n",
    "all_images_folder.mkdir(exist_ok=True)\n",
    "\n",
    "downloaded_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for idx, img_url in enumerate(sorted(all_image_urls), 1):\n",
    "    try:\n",
    "        # Get file extension\n",
    "        parsed = urllib.parse.urlparse(img_url)\n",
    "        ext = Path(parsed.path).suffix or '.jpg'\n",
    "        if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']:\n",
    "            ext = '.jpg'\n",
    "        \n",
    "        filename = f\"image-{idx:03d}{ext}\"\n",
    "        filepath = all_images_folder / filename\n",
    "        \n",
    "        # Download\n",
    "        print(f\"  [{idx}/{len(all_image_urls)}] Downloading {filename}...\", end=' ')\n",
    "        response = requests.get(img_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        rel_path = f\"images/{all_images_section}/{filename}\"\n",
    "        image_manifest[all_images_section].append({\n",
    "            'filename': filename,\n",
    "            'path': rel_path,\n",
    "            'url': img_url,\n",
    "            'size': len(response.content)\n",
    "        })\n",
    "        downloaded_count += 1\n",
    "        print(f\"‚úì ({len(response.content):,} bytes)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_count += 1\n",
    "        print(f\"‚úó Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úì Image download complete!\")\n",
    "print(f\"  Successfully downloaded: {downloaded_count}\")\n",
    "print(f\"  Failed: {failed_count}\")\n",
    "print(f\"  Location: {all_images_folder.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7fc03",
   "metadata": {},
   "source": [
    "## Extract ALL Text Content (Comprehensive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting ALL text content comprehensively...\")\n",
    "\n",
    "# Create a comprehensive text extraction\n",
    "comprehensive_md = \"# Complete Website Content\\n\\n\"\n",
    "comprehensive_md += \"This file contains ALL text content from the website, extracted in order.\\n\\n\"\n",
    "comprehensive_md += \"---\\n\\n\"\n",
    "\n",
    "# Extract all major content sections\n",
    "main_content = soup.find('body')\n",
    "\n",
    "if main_content:\n",
    "    # Get all text-containing elements in order\n",
    "    for element in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'blockquote']):\n",
    "        text = extract_text(element)\n",
    "        if text and len(text) > 5:  # Skip very short or empty elements\n",
    "            tag_name = element.name\n",
    "            \n",
    "            if tag_name in ['h1', 'h2', 'h3']:\n",
    "                # Major headings\n",
    "                level = int(tag_name[1])\n",
    "                comprehensive_md += f\"\\n{'#' * level} {text}\\n\\n\"\n",
    "            elif tag_name in ['h4', 'h5', 'h6']:\n",
    "                # Minor headings\n",
    "                level = int(tag_name[1])\n",
    "                comprehensive_md += f\"\\n{'#' * level} {text}\\n\\n\"\n",
    "            elif tag_name == 'li':\n",
    "                # List items\n",
    "                comprehensive_md += f\"- {text}\\n\"\n",
    "            elif tag_name == 'blockquote':\n",
    "                # Blockquotes\n",
    "                comprehensive_md += f\"\\n> {text}\\n\\n\"\n",
    "            else:\n",
    "                # Regular paragraphs\n",
    "                comprehensive_md += f\"{text}\\n\\n\"\n",
    "\n",
    "write_markdown('complete-content.md', comprehensive_md)\n",
    "print(\"‚úì Created complete-content.md with all text content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaae9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an all-images reference markdown file\n",
    "all_images_md = \"\"\"# All Website Images\n",
    "\n",
    "This file contains all images downloaded from the website in order.\n",
    "Use this to manually categorize images into their appropriate sections.\n",
    "\n",
    "## Images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if all_images_section in image_manifest:\n",
    "    for img_data in image_manifest[all_images_section]:\n",
    "        all_images_md += f\"### {img_data['filename']}\\n\\n\"\n",
    "        all_images_md += f\"![{img_data['filename']}]({img_data['path']})\\n\\n\"\n",
    "        all_images_md += f\"- **URL:** {img_data['url']}\\n\"\n",
    "        all_images_md += f\"- **Size:** {img_data['size']:,} bytes\\n\\n\"\n",
    "        all_images_md += \"---\\n\\n\"\n",
    "\n",
    "write_markdown('all-images.md', all_images_md)\n",
    "print(\"‚úì Created all-images.md reference file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83915a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f867e822",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
